{
  "hash": "13bec5e99ff2c967c16c907f50bc459f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Space-Time Prediction\"\nsubtitle: \"Bike Share Demand Forecasting with Panel Data & Temporal Lags\"\nauthor: \"Sihan Yu\"\ndate: today\nformat: \n  html:\n    code-fold: show\n    code-tools: true\n    toc: true\n    toc-depth: 3\n    toc-location: left\n    theme: cosmo\n    embed-resources: true\neditor: visual\nexecute:\n  warning: false\n  message: false\n---\n\n# Introduction\n\nBike share systems generate rich space–time data that can be used to support day-to-day operations such as bike rebalancing and staffing. In this assignment, I build panel-data models with temporal lags to forecast Indego bike share demand in Philadelphia.\n\nIn class, we worked through a full end-to-end workflow for Q1 2025. Here, I replicate and extend that workflow using **Q4 2024** data. This quarter spans late fall and early winter, when demand is strongly affected by colder temperatures, shorter daylight hours, and major holidays such as Thanksgiving and Christmas. After building a set of baseline models, I compare their performance to the in-class Q1 results, analyze systematic patterns in prediction errors across space, time, and demographics, and then engineer new features to improve the model. I conclude with a critical reflection on operational usefulness, equity implications, and the limitations of this modeling approach.\n\n# Part 1: Replication with a Different Quarter\n\n## 1.0 Preparation\n\nIn this section, we set up the analysis environment by loading necessary R libraries, defining visualization themes, and configuring the Census API key.\n\n### Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Core tidyverse\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Spatial data\nlibrary(sf)\nlibrary(tigris)\n\n# Census data\nlibrary(tidycensus)\n\n# Weather data\nlibrary(riem)  # For Philadelphia weather from ASOS stations\n\n# Visualization\nlibrary(viridis)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# here!\nlibrary(here)\n# Get rid of scientific notation. We gotta look good!\noptions(scipen = 999)\n```\n:::\n\n\n### Define Themes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplotTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 <- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")\n```\n:::\n\n\n### Set Census API key\n\n\n\n\n## 1.1 Load and Inspect Data\n\nHere import the Indego trip data for Q4 2024 and Philadelphia Census data. I perform a spatial join to associate stations with census tracts, filter for valid stations, and visualize the daily ridership trends to understand the data structure.\n\n### Bike Share data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindego <- read_csv(\"data/indego-trips-2024-q4.csv\")\nindego <- indego %>%\n  mutate(start_station = as.character(start_station))\n# Quick look at the data\n# glimpse(indego)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindego <- indego %>%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Look at temporal features\nhead(indego %>% select(start_datetime, interval60, week, dotw, hour, weekend))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  start_datetime      interval60           week dotw   hour weekend\n  <dttm>              <dttm>              <dbl> <ord> <int>   <dbl>\n1 2024-10-01 00:00:00 2024-10-01 00:00:00    40 Tue       0       0\n2 2024-10-01 00:06:00 2024-10-01 00:00:00    40 Tue       0       0\n3 2024-10-01 00:06:00 2024-10-01 00:00:00    40 Tue       0       0\n4 2024-10-01 00:06:00 2024-10-01 00:00:00    40 Tue       0       0\n5 2024-10-01 00:07:00 2024-10-01 00:00:00    40 Tue       0       0\n6 2024-10-01 00:08:00 2024-10-01 00:00:00    40 Tue       0       0\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Daily trip counts\ndaily_trips <- indego %>%\n  group_by(date) %>%\n  summarize(trips = n())\n\nggplot(daily_trips, aes(x = date, y = trips)) +\n  geom_line(color = \"#3182bd\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - Q4 2024\",\n    subtitle = \"Winter demand patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego bike share\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](assignments5_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThe chart illustrates the Indego bike share system's daily trip volume during Q4 2024, revealing two strong patterns. Firstly, there is a clear seasonal decline in demand, with trips steadily falling from approximately 4,500–5,000 in October to 1,000–2,000 by late December as winter approached. Secondly, a high degree of weekly volatility persists, with sharp peaks corresponding to weekdays and significant troughs for weekends or days with adverse weather conditions. This blend of decaying long-term trend and stable weekly cycles is critical for the space-time prediction models.\n\n### Load Philadelphia Census Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get Philadelphia census tracts\nphilly_census <- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\",  # White alone\n    \"B25077_001\"   # Median home value\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  progress_bar = FALSE,\n  output = \"wide\"\n) %>%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E,\n    Med_Home_Value = B25077_001E\n  ) %>%\n  mutate(\n    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %>%\n  st_transform(crs = 4326)  # WGS84 for lat/lon matching\n\n# Check the data\n# glimpse(philly_census)\n```\n:::\n\n\n### Join data to station\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create sf object for stations\nstations_sf <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to get census tract for each station\nstations_census <- st_join(stations_sf, philly_census, left = TRUE) %>%\n  st_drop_geometry()\n\n# Look at the result - investigate whether all of the stations joined to census data -- according to the map above there are stations in non-residential tracts.\n\nstations_for_map <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Add back to trip data\nindego_census <- indego %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n\n# Prepare data for visualization\nstations_for_map <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Identify which stations to keep\nvalid_stations <- stations_census %>%\n  filter(!is.na(Med_Inc)) %>%\n  pull(start_station)\n\n# Filter trip data to valid stations only\nindego_census <- indego %>%\n  filter(start_station %in% valid_stations) %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n```\n:::\n\n\n## 1.2 Adapt Code\n\nThis section focuses on data processing and panel construction. Historical weather data is fetched for the quarter, aggregate trips into a \"station-hour\" panel format, and create temporal lag features to capture demand persistence. Finally, split the data into training and testing sets based for future test.\n\n### Update weather data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweather_data <- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2024-10-01\",\n  date_end = \"2024-12-31\"\n)\n\n# Process weather data\nweather_processed <- weather_data %>%\n  mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,  # Temperature in Fahrenheit\n    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches\n    Wind_Speed = sknt  # Wind speed in knots\n  ) %>%\n  arrange(interval60, valid) %>%\n  distinct(interval60, .keep_all = TRUE) %>%\n  select(interval60, Temperature, Precipitation, Wind_Speed)\n  \n\n# Check for missing hours and interpolate if needed\nweather_complete <- weather_processed %>%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %>%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\n# Look at the weather\n#summary(weather_complete %>% select(Temperature, Precipitation, Wind_Speed))\n```\n:::\n\n\n### Aggregate to station-hour level\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count trips by station-hour\ntrips_panel <- indego_census %>%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %>%\n  summarize(Trip_Count = n()) %>%\n  ungroup()\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate expected panel size\nn_stations <- length(unique(trips_panel$start_station))\nn_hours <- length(unique(trips_panel$interval60))\nexpected_rows <- n_stations * n_hours\n\n\n# Create complete panel\nstudy_panel <- expand.grid(\n  interval60 = unique(trips_panel$interval60),\n  start_station = unique(trips_panel$start_station)\n) %>%\n  # Join trip counts\n  left_join(trips_panel, by = c(\"interval60\", \"start_station\")) %>%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# Fill in station attributes (they're the same for all hours)\nstation_attributes <- trips_panel %>%\n  group_by(start_station) %>%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop)\n  )\n\nstudy_panel <- study_panel %>%\n  left_join(station_attributes, by = \"start_station\")\n\n# Verify we have complete panel\n# cat(\"Complete panel rows:\", format(nrow(study_panel), big.mark = \",\"), \"\\n\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\nstudy_panel <- study_panel %>%\n  left_join(weather_complete, by = \"interval60\")\n```\n:::\n\n\n### Temporal Lag\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sort by station and time\nstudy_panel <- study_panel %>%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel <- study_panel %>%\n  group_by(start_station) %>%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24)\n  ) %>%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete <- study_panel %>%\n  filter(!is.na(lag1day))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split by week\n# Q1 has weeks 1-13 (Jan-Mar)\n# Train on weeks 1-9 (Jan 1 - early March)\n# Test on weeks 10-13 (rest of March)\n\n# Which stations have trips in BOTH early and late periods?\nearly_stations <- study_panel_complete %>%\n  filter(week < 51) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\nlate_stations <- study_panel_complete %>%\n  filter(week >= 51) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations <- intersect(early_stations, late_stations)\n\n\n# Filter panel to only common stations\nstudy_panel_complete <- study_panel_complete %>%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain <- study_panel_complete %>%\n  filter(week < 51)\n\ntest <- study_panel_complete %>%\n  filter(week >= 51)\n```\n:::\n\n\n## 1.3 Predictive Models\n\nFive linear regression models are built with increasing complexity, ranging from a baseline time/weather model to advanced models including temporal lags and station fixed effects. We then evaluate their performance by calculating the Mean Absolute Error (MAE) on the test set.\n\n### Model 1: Baseline(Time + Weather)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create day of week factor with treatment (dummy) coding\ntrain <- train %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) <- contr.treatment(7)\n\n# Now run the model\nmodel1 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train\n)\n```\n:::\n\n\n### Model 2: Add Temporal Lags\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n```\n:::\n\n\n### Model 3: Add Demographics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel3 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y,\n  data = train\n)\n```\n:::\n\n\n### Model 4: Add Station Fixed Effects\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel4 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station),\n  data = train\n)\n```\n:::\n\n\n### Model 5: Add Rush Hour Interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel5 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station) +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train\n)\n```\n:::\n\n\n### Predictive Models Analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create day of week factor with treatment (dummy) coding\ntest <- test %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) <- contr.treatment(7)\n\ntest <- test %>%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results <- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)\n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Mean Absolute Error by Model</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> MAE (trips) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1. Time + Weather </td>\n   <td style=\"text-align:right;\"> 0.52 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.40 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3. + Demographics </td>\n   <td style=\"text-align:right;\"> 0.63 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4. + Station FE </td>\n   <td style=\"text-align:right;\"> 0.65 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 5. + Rush Hour Interaction </td>\n   <td style=\"text-align:right;\"> 0.64 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nAnalysis of the Q4 2024 data reveals that Model 2 (Temporal Lags) is the superior model, achieving the lowest Mean Absolute Error (MAE) of 0.40. After introducing lagged variables significantly improved accuracy, the MAE lowered from 0.52 (baseline Model 1) to 0.40, confirming that recent demand history is a strong predictor of future trips. Interestingly, increasing model complexity by adding demographics (Model 3) and station fixed effects (Model 4) actually worsened performance, causing the MAE to spike to 0.63 and 0.65 respectively. This suggests that static features like station fixed effects led to overfitting on the higher-demand autumn training data, which failed to generalize to the distinct, lower-demand winter patterns observed in the test set.\n\n### 1.4 MAE comparison to Q1 2025\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(kableExtra)\n\n# Create the comparison dataframe based on your results\ncomparison_data <- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  # Your specific Q4 2024 results\n  MAE_Q4_2024 = c(0.52, 0.40, 0.63, 0.65, 0.64),\n  \n  # The provided Q1 2025 reference values\n  MAE_Q1_2025 = c(0.60, 0.50, 0.74, 0.73, 0.73)\n)\n\n# Generate the table\nkable(comparison_data, \n      digits = 2,\n      caption = \"Model Performance Comparison: Q4 2024 vs. Q1 2025\",\n      col.names = c(\"Model\", \"Q4 2024 MAE\", \"Q1 2025 MAE\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = F)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption>Model Performance Comparison: Q4 2024 vs. Q1 2025</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> Q4 2024 MAE </th>\n   <th style=\"text-align:right;\"> Q1 2025 MAE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1. Time + Weather </td>\n   <td style=\"text-align:right;\"> 0.52 </td>\n   <td style=\"text-align:right;\"> 0.60 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.40 </td>\n   <td style=\"text-align:right;\"> 0.50 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3. + Demographics </td>\n   <td style=\"text-align:right;\"> 0.63 </td>\n   <td style=\"text-align:right;\"> 0.74 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4. + Station FE </td>\n   <td style=\"text-align:right;\"> 0.65 </td>\n   <td style=\"text-align:right;\"> 0.73 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 5. + Rush Hour Interaction </td>\n   <td style=\"text-align:right;\"> 0.64 </td>\n   <td style=\"text-align:right;\"> 0.73 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nComparing our results to the Q1 2025 baseline, the Q4 models in both quarter achieved lower MAE values, the MAE increased significantly when adding static features like demographics and Station Fixed Effects (Models 3 & 4) compared to the simpler Lag model (Model 2).\n\nOur best model reaching 0.40 compared to 0.50 in Q1. This difference is likely driven by the lower overall trip volumes during the specific late-December test weeks in Q4 compared to the active winter commuting period in Q1. Temporally, Q4 differs significantly from Q1 by exhibiting a steep seasonal decline from October to December, as opposed to the stable low-demand baseline of deep winter. despite these seasonal differences, Temporal Lags remained the most critical feature in our quarter. However, unlike Q1, our Q4 model relied more heavily on weather variables to capture the dramatic transition from comfortable autumn temperatures to freezing winter conditions.\n\n# Part 2: Error Analysis\n\n## 2.1 Spatial patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- test %>%\n  mutate(\n    error = Trip_Count - pred2,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour < 7 ~ \"Overnight\",\n      hour >= 7 & hour < 10 ~ \"AM Rush\",\n      hour >= 10 & hour < 15 ~ \"Mid-Day\",\n      hour >= 15 & hour <= 18 ~ \"PM Rush\",\n      hour > 18 ~ \"Evening\"\n    )\n  )\n\n\n## Create Two Maps Side-by-Side with Proper Legends \n# Calculate station errors\nstation_errors <- test %>%\n  filter(!is.na(pred2)) %>%\n  group_by(start_station, start_lat.y, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.y), !is.na(start_lon.y))\n\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.y, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE\\n(trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),  # Fewer, cleaner breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\",\n       subtitle = \"Higher in Center City\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand\np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.y, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg\\nDemand\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),  # Clear breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\",\n       subtitle = \"Trips per station-hour\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine with better layout\nlibrary(gridExtra)\nlibrary(grid)\n\ngrid.arrange(\n  p1, p2, \n  ncol = 2,\n  top = textGrob(\n    \"Model 2 Performance: Errors vs. Demand Patterns\",\n    gp = gpar(fontsize = 16, fontface = \"bold\")\n  )\n)\n```\n\n::: {.cell-output-display}\n![](assignments5_files/figure-html/spatial_errors-1.png){width=672}\n:::\n:::\n\n\n**Interpretation**\\\nThe error maps show a clear link between demand and prediction errors. The areas with the highest MAE, colored orange and red, directly match the areas with the highest average demand. These errors are focused in Center City and University City. This shows the model struggles with the high volume of trips in these busy areas. The demand in these zones is often spontaneous, which makes it much harder to predict compared to quieter residential neighborhoods.\n\n## 2.2 Temporal patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# MAE by time of day and day type\ntemporal_errors <- test %>%\n  group_by(time_of_day, weekend) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](assignments5_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n**Interpretation**\\\nThe temporal analysis highlights distinct challenges during peak times. The bar chart clearly indicates that the model struggles the most during the Weekday PM Rush. The Mean Absolute Error (MAE) reaches almost 0.7 trips at this time, which is much higher than during the AM Rush or midday. This suggests that afternoon travel behavior is simply more unpredictable. People may run errands or attend social events after work, which is less regular than the fixed morning commute. On the other hand, the model performs best overnight when demand is consistently low.\\\n\n## 2.3 Demographic patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Join demographic data to station errors\nstation_errors_demo <- station_errors %>%\n  left_join(\n    station_attributes %>% select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),\n    by = \"start_station\"\n  ) %>%\n  filter(!is.na(Med_Inc))\n\n# Create plots\np1 <- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Errors vs. Median Income\", x = \"Median Income\", y = \"MAE\") +\n  plotTheme\n\np2 <- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Transit Usage\", x = \"% Taking Transit\", y = \"MAE\") +\n  plotTheme\n\np3 <- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Race\", x = \"% White\", y = \"MAE\") +\n  plotTheme\n\ngrid.arrange(p1, p2, p3, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](assignments5_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n**Interpretation**\\\nThe scatterplots show an interesting pattern between neighborhood data and errors.\n\n-   Income: There is a positive trend with Median Income. As income increases, the prediction error also increases.\n\n-   Race: Similarly, neighborhoods with a higher percentage of white residents show higher errors.\n\n-   Transit: Areas with high public transit use have lower prediction errors.\n\nThis pattern occurs because wealthy, white neighborhoods often have the busiest bike share stations. The higher error is simply caused by the higher volume of riders. It does not reflect the model's ability to understand specific populations.\n\n# Part 3: Feature Engineering & model improvement\n\nThe new features chosen were Holiday and Feels Like Temperature. These were added to the best performing lag model, which was Model 2.\n\n**Holiday**: Bike share demand changes drastically on major holidays like Thanksgiving or Christmas. These events are strong temporal factors. The model needs a specific dummy variable to account for these massive shifts in rider behavior.\n\n**Feels Like Temperature** : Since the quarter is Q4 (moving into winter), wind chill becomes a major factor. The \"Feels Like\" Temperature (calculated from actual temperature and wind speed) is a better predictor of rider comfort and willingness to ride than the raw air temperature. A 40°F day with high wind feels significantly colder and discourages riding more than a calm 40°F day.\n\n**7-Day Rolling Average**: Here calculated a rolling mean of trip counts over the previous 7 days (168 hours) for each station to act as a demand \"smoother\". By incorporating the 7-day rolling average, we capture the station's recent baseline demand trend. This allows the model to distinguish between random spikes and genuine shifts in usage, such as a station generally trending busier over the last week. In this way, a more stable signal is provided than single-point lags alone.\n\n## 3.1 My Model: model 4 + new features\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(zoo)\n\n# holidays\nholiday_dates <- as.Date(c(\n  \"2024-11-28\",  # Thanksgiving\n  \"2024-11-29\",  # Black Friday\n  \"2024-12-24\",  # Christmas Eve\n  \"2024-12-25\",  # Christmas Day\n  \"2024-12-31\"   # New Year's Eve\n))\n\n\n# 在 study_panel_complete 基础上加新特征\nstudy_panel_fe <- study_panel_complete %>%\n  arrange(start_station, interval60) %>%\n  group_by(start_station) %>%\n  mutate(\n    # 7 day rolling\n    rolling7 = rollmean(Trip_Count, k = 7 * 24, fill = NA, align = \"right\")\n  ) %>%\n  ungroup() %>%\n  mutate(\n    rolling7 = replace_na(rolling7, 0),\n    holiday = ifelse(date %in% holiday_dates, 1, 0),\n    wind_mph = Wind_Speed * 1.15078,\n    feels_like = ifelse(\n      Temperature <= 50 & wind_mph > 3,\n      35.74 + 0.6215 * Temperature -\n        35.75 * (wind_mph^0.16) +\n        0.4275 * Temperature * (wind_mph^0.16),\n      Temperature\n    )\n  )\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_fe <- study_panel_fe %>%\n  filter(week < 51)\n\ntest_fe <- study_panel_fe %>%\n  filter(week >= 51)\n\n# day-of-week \ntrain_fe <- train_fe %>%\n  mutate(dotw_simple = factor(dotw,\n                              levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\ntest_fe <- test_fe %>%\n  mutate(dotw_simple = factor(dotw,\n                              levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Treatment coding\ncontrasts(train_fe$dotw_simple) <- contr.treatment(7)\ncontrasts(test_fe$dotw_simple)  <- contr.treatment(7)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Baseline model on feature-engineered data: same structure as original Model 2\nmodel2_fe <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple +\n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train_fe\n)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model 6: Model 2 + (holiday, feels_like, rolling7)\nmodel6 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple +\n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    holiday + feels_like + rolling7,\n  data = train_fe\n)\n```\n:::\n\n\n## 3.2 MAE comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ---- Predict using Model 4 and Model 6 ----\n# Predictions on test_fe\ntest_fe$pred2 <- predict(model2_fe, newdata = test_fe)\ntest_fe$pred6 <- predict(model6,    newdata = test_fe)\n\n# Compute MAE with na.rm = TRUE\nmae2 <- mean(abs(test_fe$Trip_Count - test_fe$pred2), na.rm = TRUE)\nmae6 <- mean(abs(test_fe$Trip_Count - test_fe$pred6), na.rm = TRUE)\n\nmae_comparison <- data.frame(\n  Model = c(\"Model 2_fe: Time + Weather + Lags\",\n            \"Model 6: Model 2 + Holiday + Feels-like + Rolling7\"),\n  MAE   = c(mae2, mae6)\n)\n\nmae_comparison %>%\n  kable(digits = 3,\n        caption = \"MAE Comparison: Baseline Model 2_fe vs. Improved Model 6\") %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>MAE Comparison: Baseline Model 2_fe vs. Improved Model 6</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> MAE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Model 2_fe: Time + Weather + Lags </td>\n   <td style=\"text-align:right;\"> 0.397 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Model 6: Model 2 + Holiday + Feels-like + Rolling7 </td>\n   <td style=\"text-align:right;\"> 0.394 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe addition of the holiday and feels_like features successfully improved model performance. The Mean Absolute Error (MAE) decreased from \\[MAE 2 value\\] in the baseline Model 2 to \\[MAE 6 value\\] in the improved Model 6.\n\nThese features likely improved predictions where weather and holidays are key. The feels_like feature improves prediction accuracy on cold or windy days. The holiday feature specifically improves accuracy on high-impact dates like Thanksgiving, where demand patterns completely shift away from a standard Thursday schedule.\n\n**Try a poisson model for count data**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_pois <- glm(\n  Trip_Count ~ as.factor(hour) + dotw_simple +\n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    holiday + feels_like + rolling7,\n  data = train_fe,\n  family = poisson(link = \"log\")\n)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_fe$pred_pois <- predict(model_pois, newdata = test_fe, type = \"response\")\nmae_pois <- mean(abs(test_fe$Trip_Count - test_fe$pred_pois), na.rm = TRUE)\nmae_pois\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3981328\n```\n\n\n:::\n:::\n\n\nUnfortunately, the Poisson model which is theoretically better suited for non-negative count data than standard linear regression did not improve prediction performance. However, this approach yielded an MAE of 0.398, which was slightly inferior to both the linear baseline (0.397) and our feature-engineered model (0.394).This outcome likely happened bacause for high-demand stations, the trip volume is sufficiently large that the data approximates a normal distribution, allowing the linear model to remain highly robust. Consequently, the added complexity of the Poisson link function did not provide a tangible benefit over the strong predictive power of the linear autoregressive lags in this specific context.\n\n# Part 4: Critical Reflection\n\n1.  **Operational implications & limitations:** Achieving a final MAE of 0.394 is encouraging, as it implies an average deviation of less than half a trip per station-hour, serving as a reliable baseline for fleet scheduling. However, this overall accuracy masks a critical time sensitivity: the model is weakest during the PM Rush, where underestimating demand leads to a serious problem that commuters find full stations with no place to dock. Thus, I suggest treating the model not as an \"autopilot\" solution, but as a decision support tool to assist dispatchers rather than replace them. Furthermore, a key limitation is the assumption of infinite vehicle supply; the model predicts latent demand rather than realized trips. Future improvements should integrate real-time \"bikes available\" data and account for spatial spillover effects to better reflect operational reality.\n\n2.  **Equity considerations:**\\\n    From an equity perspective, absolute errors are significantly higher in Center City and high-income neighborhoods. While this is primarily a \"volume bias\" rather than algorithmic discrimination, relying solely on these predictions for rebalancing could unintentionally prioritize high-traffic areas and marginalize lower-demand or low-income communities. To prevent this, I recommend that Indego to set a \"minimum service level\" to guarantee baseline inventory regardless of predicted demand, and monitor the possible relative error to ensure consistent performance across demographics. Low demand does not mean unimportant. Efficiency must be balanced with equity to ensure the system benefits all citizens.\n",
    "supporting": [
      "assignments5_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}